{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "After this assignment was released, it seems Google has raised the minimum version of CUDA/CUDNN available on free Colab instances, which means we have to now manually downgrade both CUDA and CUDNN. We apologize for the inconvenience!\n",
        "\n",
        "Before you connect to a runtime, please sign up for an NVIDIA developer account (which is free) so that you can download CUDNN7. Unfortunately we are not allowed to redistribute it due to licensing issues. \n",
        "\n",
        "Once you are signed up and logged in, use this link to download CUDNN7 locally:\n",
        "\n",
        "[Download cuDNN v7.6.5 (November 5th, 2019) Runtime Library for Ubuntu18.04 (Deb), for CUDA 10.0](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.0_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.0_amd64.deb) (165MB)\n",
        "\n",
        "which is found on https://developer.nvidia.com/rdp/cudnn-archive\n",
        "\n",
        "**The above link ensures you are downloading the correct version of CUDNN7**\n",
        "\n",
        "If you need to download it manually, then you will need to select v7.6.5 for CUDA 10.0 and then select cuDNN Runtime Library for Ubuntu18.04 (Deb), but if you are logged in, the above link should work.\n",
        "\n",
        "Once you have downloaded it locally, you will then need to upload it to the Colab instance and install it, as instructed below. For now, once it is saved locally to your computer, proceed to the next step, which is changing the runtime type to GPU in the runtime menu. \n",
        "\n"
      ],
      "metadata": {
        "id": "l_TNGkB4tndL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Note: when you aren't actively using the GPU for training, colab can be quick to disconnect you, so you might want to read through this notebook and familiarize yourself with the steps you'll take before you connect to a runtime/change the runtime type. Also note that if you upload data to Colab directly (and don't use Google Drive), and then change the runtime type, you will have to reupload your data. It may be easier to use Google Drive: see [here for tips on how to do so](https://colab.research.google.com/notebooks/io.ipynb).**\n",
        "\n",
        "When you are ready to train, start by changing the runtime type to 'GPU' in the Runtime menu.\n",
        "\n",
        "Next, we need to install Tensorflow V1 in order to run the fine-tuning code. The below cell uninstalls TF2 and installs TF1. \n"
      ],
      "metadata": {
        "id": "qggLvtj5pkK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-FlA3WoV6GV"
      },
      "outputs": [],
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow-gpu==1.15"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you install TF version 1.15.0 using the above cell, and click the \"RESTART RUNTIME\" button in the output of the previous cell (this button doesn't always show up automatically) or by using the Runtime menu after the previous cell finishes executing."
      ],
      "metadata": {
        "id": "UFfk465Qp4_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "assert(tf.VERSION == \"1.15.0\") #Make sure you install Tensorflow 1.15.0 and restart the runtime!"
      ],
      "metadata": {
        "id": "wZQvNHV0qGxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time to downgrade CUDA to 10.0 (which is compatible with TF1.15). Run the following cell and when it successfully completes, **RESTART THE RUNTIME AGAIN**\n",
        "\n",
        "Note that this cell can take a painfully long time to complete. In particular it might spend a few minutes on the following step (compiling a new kernel module):\n",
        "\n",
        "`Building initial module for 4.15.0-193-generic`\n",
        "\n",
        "While it is running you can start uploading `libcudnn7_7.6.5.32-1+cuda10.0_amd64.deb` (read the text cell two steps below this one for details)"
      ],
      "metadata": {
        "id": "CbeBtWWRv1Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Uninstall the current CUDA version\n",
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update\n",
        "\n",
        "#Download CUDA 10.0\n",
        "!wget  --no-clobber https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "#install CUDA kit dpkg\n",
        "!dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "!sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-10-0"
      ],
      "metadata": {
        "id": "HdqVWxMrtWVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After you have restarted the runtime again** check to make sure `nvcc` reports the correct version of CUDA is installed:"
      ],
      "metadata": {
        "id": "k0bJI7kAwMBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "ytRj8gm8wKNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now upload `libcudnn7_7.6.5.32-1+cuda10.0_amd64.deb` to Colab (you can click on the 'Files' button on the left and drag it in there) and **after it has finished uploading** (which can take a long time depending on your connection speed plus how busy the Colab instance is), run the following cell to install CUDNN7 (you will **not** need to restart the runtime after installing CUDNN7 - whew!)\n",
        "\n",
        "Thanks for your patience! Now TF 1.15 is supported again!"
      ],
      "metadata": {
        "id": "nqYUlYgzwOG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dpkg -i libcudnn7_7.6.5.32-1+cuda10.0_amd64.deb"
      ],
      "metadata": {
        "id": "VleQvd_2zdYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can verify `libcudnn7` is present by running the following cell (it's ok it libcudnn8 is also present, but you should see `7.6.5.32-1+cuda10.0` in the list below)"
      ],
      "metadata": {
        "id": "3zSQ-9Vmzy3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dpkg -l | grep -i cudnn"
      ],
      "metadata": {
        "id": "Gz-atq2Lztin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install another dependency we will need for training:"
      ],
      "metadata": {
        "id": "WnMD4to02usi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_slim"
      ],
      "metadata": {
        "id": "NA431FMw2szI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll clone the homework assignment and change into the cloned directory"
      ],
      "metadata": {
        "id": "4SZOXREvsAVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mmaz/cs249r-a2\n",
        "import os\n",
        "os.chdir(\"/content/cs249r-a2\")\n",
        "!pwd"
      ],
      "metadata": {
        "id": "p2fDrDszo6eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, upload your image data into two folders under the folder `custom_dataset/TL_Datasets` - these folders are hardcoded, but you will create two folders within `TL_Datasets` with names corresponding to category labels you choose. \n",
        "\n",
        "You need to make sure for each category, there is a separate folder. For example, if you are downloading images of people not wearing masks, you will save these images in the `absent mask` folder (you can choose whatever name you want!) folder. Likewise, if you download images with a person wearing a mask, you will save it `mask` folder. Once you have images for each category in their respective folders, keep the parent folder named `TL_Datasets` (the conversion script expects this name!). Your folder tree in colab should resemble something like this:\n",
        "\n",
        "  ```\n",
        "├── content\n",
        "│   └── cs249r-a2\n",
        "│       └── custom_dataset\n",
        "│           └── TL_Datasets\n",
        "│               ├── absent_mask\n",
        "│               └── mask\n",
        "\n",
        "  ```\n",
        "\n",
        "  If uploading your data to Colab is taking a long time, see the tips section in the Assignment 2 writeup on compressing your images before uploading.\n",
        "\n",
        "**Dataset Preparation**\n",
        "\n",
        "Once you have downloaded the dataset and a folder structure as described above, run the next cell to convert your data into a set of sharded `train` and `val` TF Records.\n",
        "\n",
        "**Note:** you can modify a hardcoded parameter in the script (`datasets/download_and_convert_custom.py`) before running the cell to change the number of validation images saved as TF records. A decent rule of thumb is 10% or 20% of your data (so 350 implies a finetuning set of ~1750 images following this rule). \n",
        "\n",
        "```python\n",
        "# The number of images in the validation set.\n",
        "_NUM_VALIDATION = 350\n",
        "```\n",
        "\n",
        "Lastly, keep in mind how balanced your dataset is between your two classes. \n",
        "\n",
        "The script shuffles samples at random."
      ],
      "metadata": {
        "id": "IdsAGh4exJOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python datasets/download_and_convert_data.py \\\n",
        "    --dataset_name=custom \\\n",
        "    --dataset_dir=/content/cs249r-a2/custom_dataset/"
      ],
      "metadata": {
        "id": "vp3t_aU9eWcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If the script runs successfully, then you should see output similar to:\n",
        "\n",
        "  ```\n",
        "  >> Converting image 134/558 shard 1\n",
        "  >> Converting image 350/350 shard 4\n",
        "\n",
        "  Finished converting the dataset!\n",
        "  ```\n",
        "\n",
        "Use the below cell to inspect the TL_Datasets folder, where you will find train and validation TFRecord files are generated. Also, you will find a `labels.txt` file, which will have class labels. Depending upon how you organized your folders, you will find that `0` and `1` class labels will be assigned to that category (e.g., since ‘absent_mask’ is lexically lower than ‘mask’ it gets a label id of 0). This should help you interpret/modify your LED light behavior and serial output in [`arduino_detection_responder.cpp`](https://github.com/tinyMLx/arduino-library/blob/main/examples/person_detection/arduino_detection_responder.cpp)"
      ],
      "metadata": {
        "id": "xYQFOxxx2CsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls custom_dataset/\n",
        "!echo \"\" && cat custom_dataset/labels.txt"
      ],
      "metadata": {
        "id": "ob7Xd2lNlDvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are ready to start transfer learning! The below cell will start executing the transfer learning process on top of a pre-trained checkpoint (this checkpoint was trained on Visual Wake Words for 1 million steps). You can modify the below cell to change the frequency of saved checkpoints to your liking (it defaults to every 200 seconds), but keep an eye on the training process so that you don't leave it running for too long (colab will eventually disconnect you). When using a GPU runtime you should be able to get ~5000 steps trained within 20 minutes or so, give or take, and not face any disconnection risks.\n",
        "\n",
        "When you think your model has been trained enough, you can click `Runtime -> Interrupt Execution` to kill training. Then you can move on to evaluation. If you find the model wasn't trained enough, you can either collect more data, train for longer, or both. If you suspect you are overfitting, either train for fewer steps (or use an earlier checkpoint) or collect more data.\n",
        "\n",
        "By default, checkpoints will be saved to `/content/cs249r-a2/transferlearning/`\n",
        "\n",
        "The argument `--dataset_name=visualwakewords` in this cell and cells below is due to hardcoded behavior in the training script - you can leave this as is (or you can update the training script if you prefer).\n",
        "\n",
        "You can experiment with changing `--checkpoint_exclude_scopes`, and `--trainable_scopes` flags. For `--checkpoint_exclude_scopes`, you will list the node names (separated by comma) that you exclude when loading the pre-trained weights. `--trainable_scopes` means which layer you want to train. Obviously, this value should be the same as what you passed for the `--checkpoint_exclude_scopes` parameter since it is only for those layers you are not initializing the weights from the pre-trained model. For MobileNet, `MobilenetV1/Logits, MobilenetV1/Predictions` corresponds to the last two layers. However, if you want to play with other layers, you can use [Netron](https://netron.app/) to visualize the node names. So except for these nodes, the weights from the pre-trained model will be initialized in the model.\n",
        "\n",
        "For the other hyperparameters, always start with the hyperparameters below that are known to work. We want you to appreciate that hyperparameters are important for making any neural network work. But if you start tweaking these (esp for computer vision tasks), two weeks (duration of this assignment) is insufficient for this endevour (unless you are extremely lucky!). So choose your battles wisely!"
      ],
      "metadata": {
        "id": "fRQCamXn2-kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_image_classifier.py \\\n",
        "    --train_dir=transferlearning/  \\\n",
        "    --dataset_dir=/content/cs249r-a2/custom_dataset/ \\\n",
        "    --dataset_name=visualwakewords \\\n",
        "    --dataset_split_name=train \\\n",
        "    --preprocessing_name=mobilenet_v1 \\\n",
        "    --model_name=mobilenet_v1_025 \\\n",
        "    --train_image_size=96 \\\n",
        "    --save_summaries_secs=200 \\\n",
        "    --save_interval_secs=200 \\\n",
        "    --max_number_of_steps=1005000 \\\n",
        "    --learning_rate=0.045 \\\n",
        "    --label_smoothing=0.1 \\\n",
        "    --learning_rate_decay_factor=0.98 \\\n",
        "    --num_epochs_per_decay=2.5 \\\n",
        "    --moving_average_decay=0.9999 \\\n",
        "    --use_grayscale=True \\\n",
        "    --checkpoint_path=ckpts/person_detection/model.ckpt-1000000 \\\n",
        "    --checkpoint_exclude_scopes= MobilenetV1/Logits,MobilenetV1/Predictions \\\n",
        "    --trainable_scopes= MobilenetV1/Logits,MobilenetV1/Predictions\n"
      ],
      "metadata": {
        "id": "-Pe0IsyRiajg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell will evaluate your trained model on your validation set. This can be used as feedback to guide your collection of additional data and retraining the model (e.g., how many steps are ideal). \n",
        "\n",
        "Change the `--checkpoint` argument to point to the checkpoint you wish to evaluate in the `transferlearning/` folder.\n",
        "\n",
        "**Remember to report the final validation accuracy you achieve in your Assignment 2 writeup**\n",
        "\n",
        "The accuracy will be reported as (for example):\n",
        "\n",
        "```\n",
        "eval/Accuracy[0.75714287]\n",
        "```"
      ],
      "metadata": {
        "id": "jOgnWmct5VvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python eval_image_classifier.py \\\n",
        "    --alsologtostderr \\\n",
        "    --checkpoint_path=transferlearning/model.ckpt-1129 \\\n",
        "    --dataset_dir=/content/cs249r-a2/custom_dataset/ \\\n",
        "    --dataset_name=visualwakewords \\\n",
        "    --dataset_split_name=val \\\n",
        "    --model_name=mobilenet_v1_025 \\\n",
        "    --preprocessing_name=mobilenet_v1 \\\n",
        "    --use_grayscale=True \\\n",
        "    --train_image_size=96"
      ],
      "metadata": {
        "id": "O5sdXOootR4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next steps will help you convert your model to a C++ source file so that you can load it on the Arduino Nano 33 BLE. \n",
        "\n",
        "First, export the model graph by running the following cell. This command will create the graph definition for the neural network model that you used (MobileNetV1). Unless you change the name of the `---output_file`, the name of the graph def will be `vww_96_grayscale_graph`. (it's fine to leave it as is).\n",
        "\n",
        "This does not contain your model weights, just placeholders for where your weights belong (which you'll fill in using the cell after this one)"
      ],
      "metadata": {
        "id": "h-ioCwYO_ri8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python export_inference_graph.py \\\n",
        "    --alsologtostderr \\\n",
        "    --dataset_name=visualwakewords \\\n",
        "    --model_name=mobilenet_v1_025 \\\n",
        "    --image_size=96 \\\n",
        "    --use_grayscale=True \\\n",
        "    --output_file=vww_96_grayscale_graph.pb"
      ],
      "metadata": {
        "id": "J1YgYMWZ8r0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell will freeze your weights into a new protobuf. This command will store the weights values as constants in the corresponding nodes in the graph definition you generated above. Also in this step, the script will prune the nodes that are not needed in the forward inference (e.g., nodes that are needed for gradient computation in backpropagation are all pruned from the graph). The parameters in the script are explained below:\n",
        "\n",
        "\n",
        "  `--input_graph` is the name of the graph def file that you saved above.\n",
        "\n",
        "  `--input_checkpoint` is the model name of your final trained model. \n",
        "  \n",
        "  **Change `--input_checkpoint` to point to your desired checkpoint**\n",
        "\n",
        "  `--output_graph` is the name of the frozen graph. If you don't change the parameter, by default, it will be saved as `person_detection_frozen.pb`\n",
        "\n",
        "  `--output_node` is the name of the last node in your neural network model. We are using MobileNetv1, so the node name will be `MobilenetV1/Predictions/Reshape_1`"
      ],
      "metadata": {
        "id": "7Li6uk3u84GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python freeze_graph.py \\\n",
        "    --input_graph=vww_96_grayscale_graph.pb \\\n",
        "    --input_checkpoint=transferlearning/model.ckpt-5000 \\\n",
        "    --input_binary=true \\\n",
        "    --output_graph=person_detection_frozen.pb \\\n",
        "    --output_node_names=MobilenetV1/Predictions/Reshape_1"
      ],
      "metadata": {
        "id": "Fvrh2uHA9J6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to create a TFLite file by quantizing the model. This script will output `person_detection.tflite` (the output filename is hardcoded - as usual, you can leave it as is).\n",
        "\n",
        "In particular, this script quantizes the weights to an int8 format. For effective quantization, you need to provide a representative input sample so the quantization script can monitor the range, which it will use it during quantization. Use the `--sample_data` flag to point to a representative input dataset. By default, it points to the first shard of your training data. The second input (`--frozen_model`) to the quantization script is the frozen model that you created in the previous step."
      ],
      "metadata": {
        "id": "HbXRo1rR9GXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python quantize_weights.py --sample_data custom_dataset/train.record-00000-of-00005 --frozen_model person_detection_frozen.pb"
      ],
      "metadata": {
        "id": "5JGqKYo79vMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have generated the TFLite file, you will convert the TFLite file to a C++ source file to deploy it on the Arduino Nano 33 BLE using `xxd` in the following command.\n",
        "\n",
        "If you haven't heard of `xxd` before, the following pages offer a brief explanation:\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Hex_dump\n",
        "* https://manpages.ubuntu.com/manpages/bionic/en/man1/xxd.1.html\n",
        "\n"
      ],
      "metadata": {
        "id": "AMZziHQHKQeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!xxd -i person_detection.tflite person_detect_model_data.cpp"
      ],
      "metadata": {
        "id": "I051y_yG-kZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, download `person_detection.tflite` and `person_detect_model_data.cpp` to your local computer and import it into your arduino sketch as described in the Assignment 2 writeup. \n",
        "\n",
        "You will need the `tflite` file in order to calculate the TensorFlow Lite Micro arena size. The `.cpp` file contains your serialied TFLM model definition and weights."
      ],
      "metadata": {
        "id": "xVirrMj3K3WT"
      }
    }
  ]
}